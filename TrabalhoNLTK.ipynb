{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrabalhoNLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1gDPiQdsb2QyVJ3PnJr0eB5uMe8Ey_vBw",
      "authorship_tag": "ABX9TyOsr+6SAHnGFGRvny79clMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandokj/nlp-sentiment-analyzer/blob/master/TrabalhoNLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JF6kNPDYhpp"
      },
      "source": [
        "# Trabalho NLTK Orlando Krause"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Tnk6MKaX4T"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmdIrQNEnddF",
        "outputId": "1b002d60-4b3b-40a4-e8f7-394e4a1baab6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0JgGQ3BeFkc"
      },
      "source": [
        "# Foi feito um replace usando expressão regular para padronizar a separação entre texto e sentimento:\n",
        "Expressao: ***,(negative|positive|sentiment)\"?;;;;?;?;?***  \n",
        "Replace: ***#;#$1***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me2io0ikaoDl",
        "outputId": "b57f3267-b921-4292-ba10-52d379f8e6b4"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/imdb.csv\", sep=\"#;#\")\n",
        "df.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBajJjpzUAPA"
      },
      "source": [
        "# Dividimos o dataset em treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ9yYiNVqKYn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=df.review.values\n",
        "Y=df.sentiment.values\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size=0.3, stratify=Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4bhwAl7UH6Y"
      },
      "source": [
        "# Definimos a função tokenizadora, usando stemmer e filtrando stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT9Wsteuf3jd"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "enStopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize_text(text):\n",
        "  words = word_tokenize(text.replace(\"<br />\", \"\").replace(\".\", \" \").replace(\"n't\", ' not').replace(\"'ll\", ' will'), \"english\")\n",
        "  filteredWords = filter(lambda word: word not in enStopWords, map(lambda word: stemmer.stem(word.lower()), words))\n",
        "  return list(filter(lambda word: re.search(\"[a-z']\", word) is not None, filteredWords))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaH-SvdXxe6u"
      },
      "source": [
        "# Aqui demora uns 5 minutos para transformar os reviews em um vetor usando a nossa função *tokenizer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EqiiL9hAB1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56bca95-1991-4f50-8ecf-15a6688476f6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer(tokenizer=tokenize_text)\n",
        "vec.fit(X_train)\n",
        "X_train_vector=vec.transform(X_train)\n",
        "X_test_vector=vec.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQFGdg67UqdP"
      },
      "source": [
        "# Treinamos o classificador com o vetor criado anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO2xv_hrAJPH",
        "outputId": "fdecc54e-f490-4571-8b28-4f5faa017bbf"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_vector, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iihkpgMeVCgR"
      },
      "source": [
        "# Realizamos a previsão com o dataset de testes e salvamos o resultado em um data frame junto com o Ground Truth(gt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEgpDIozRg15"
      },
      "source": [
        "lr_predict_result = lr.predict(X_test_vector)\n",
        "result_df = pd.DataFrame(data={'gt': Y_test, 'lr': lr_predict_result})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mD6Td2SAqrw"
      },
      "source": [
        "# Aqui começa o sentiment analyzer do NLTK, consideramos positivo quando o score de positivo for maior que o score de negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvwBV_NBPZta",
        "outputId": "b09d0e70-281a-4d35-a4fc-dbe3ae99814b"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sia_classify(text):\n",
        "  result = sia.polarity_scores(text)\n",
        "  # Positivo se o score de positivo for maior que o score de negativo\n",
        "  return 'positive' if result['pos'] > result['neg'] else 'negative'\n",
        "\n",
        "result_df['sia'] = list(map(sia_classify, X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KbTO3wMSq9a"
      },
      "source": [
        "# Comparando acurácias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgjEYArxRg81",
        "outputId": "9dc86f01-4735-4eca-e6b3-2f3dfbd8b26a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "lr_score = accuracy_score(result_df['gt'].values, result_df['lr'].values)\n",
        "sia_score = accuracy_score(result_df['gt'].values, result_df['sia'].values)\n",
        "\n",
        "print(f'Acurácia da regressão logística: {lr_score:.2f}')\n",
        "print(f'Acurácia do SentimentIntensityAnalyzer: {sia_score:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia da regressão logística: 0.89\n",
            "Acurácia do SentimentIntensityAnalyzer: 0.69\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}